# Implement copilot meta API
Copilot meta API provides ways for copilot client to get the current state of user interaction in JSON, as well as 
execute actions encoded in JSON. Clearly, the actual definition of state and action and their semantics are 
application dependent, and need to be designed for the use cases that your copilot try to address.  so Before starting 
to develop the copilot, product manager and architect need to work together to design the data
structure needed to capture the state of user interaction, as well as a set of actions that your app can execute per 
user's request.

## Expose user interaction state
To help user in a context dependent fashion, copilot needs to be aware the current state of user interaction with 
your app, and use these state as the context for conversational interaction. Copilot can then suggest 
different action for the same command under different context, instead of requiring user to provide all the details 
all the time, thus providing a more natural experience.

For modern GUI application, the state of user interaction can be represented by a stack of pages, with each page
defined by a type, which defines what information it can capture from user, and instance of that type which 
represent the user input so far on that page. It should be clear that page on top is expanded from a component on
the bottom page. In general, the state should simply capture the snapshot of the interaction, instead of the history 
of it, in order to save the storage need to save them.

Taking OpenCUI copilot as an example, the context primarily consists of the organization label, agent label,  agent 
type, page label, and so on. For **frontend developers**, you can refer to the [sending message](./opencui-sdk.
md#send-messages) section  to learn how to pass the context, while **copilot builders** can refer to [building 
PageContext frame](./build-copilot.md#build-a-frame) to learn how to declare the context on the OpenCUI platform.
```json
{
    "orgLabel":"me.test",
    "agentLabel":"pingpongSL",
    "agentType":"chatbot",
    "lang":"en",
    "page":"typeList"
}
```

### Copilot response
Upon receiving the user input along with the context that they are in, copilot can provide help in two ways: 
information and action in form of buttons. The action button captures user intention, and once clicked, will change 
app state as if users have interacted with app (by mouse clicking and keyboard inputting) in the way that can achieve 
user's goal. 

For example, let's consider OpenCUI platform. If users want to clone a chatbot, they can create a new chatbot by  
selecting the cloning option, which is an action. Additionally, if the users are not currently on the 
appropriate  page to perform this action, they need to direct to the right page first, which is also an action.  For 
**frontend developers**, you can refer to the [action](./opencui-sdk.md#action)  section to learn the data structure 
required,  while **copilot builders** can look up the [universal messages](https://opencui.io/reference/channels/universalmessage.html#json-representation) to grasp the basic format of actions. 
The "actionParams" field in the action date type and the "payload" field in universal messages need to be determined 
collaboratively between frontend developers and the copilot builders. Taking the previous example into consideration,
the "actionParams" ("payload") field for the clone action can be structured as follows:

```json
{
    "clickAction":"custom",
    "targetAction": {
        "action":"createProject",
        "projectType": "chatbot"
    }
}
```

The set of actions your copilot can expose should be supported by your implementation of copilot meta API, and these 
json object can be easily generated by copilot backend, and rendered in copilot frontend. Notice copilot frontend 
only need to render these action encoded in json, which does not require any understanding of it. 